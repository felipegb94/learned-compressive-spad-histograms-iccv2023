#!/usr/bin/env bash

#SBATCH --array=1-20%10

#SBATCH --partition=research
#SBATCH --cpus-per-task=8
#SBATCH --gres=gpu:1
#SBATCH --mem=28000
#SBATCH --time=96:0:0

###SBATCH --nodelist=euler17
###SBATCH --exclude=euler09
#SBATCH --exclude=euler18,euler04,euler05,euler09,euler21,euler24,euler25,euler26,euler27

###SBATCH -o slurm.%j.%N.out # STDOUT
###SBATCH -e slurm.%j.%N.err # STDERR
#SBATCH -o logs/slurm.%A.%a.%N.out
#SBATCH -e logs/slurm.%A.%a.%N.err

#SBATCH --job-name=penlocal
#SBATCH --no-requeue

## If any command fails exit.
set -e 
sleep $((RANDOM%4))

module load anaconda/mini/4.9.2
bootstrap_conda
conda activate csphenv38man

test_dataset=sunrgbd
train_dataset=nyuv2_64x64x1024_80ps
#START=$1
#END=$2
START=$((($SLURM_ARRAY_TASK_ID-1)*100))
END=$((($SLURM_ARRAY_TASK_ID)*100))
 
model_name=DDFN_C64B10/norm-none/loss-kldiv_tv-1e-05
experiment_name=no_compression_baselines_lr-1e-4_check
model_dirpath=outputs/${train_dataset}/${experiment_name}/${model_name}/run-complete_2023-07-02_122446
ckpt_id=epoch=29-step=103890-end-of-epoch.ckpt # 72 images == 0.01700 | 128 images == 0.01756 | 128 images large depth (offset 7m): 0.08526 | 128 images masked tbins (9m): 0.01757
#python test.py dataset='"'$test_dataset'"' ++model_name='"'$model_name'"' ++experiment_name=$experiment_name ++model_dirpath='"'$model_dirpath'"' ++ckpt_id='"'$ckpt_id'"' ++train_dataset='"'$train_dataset'"' ++start_idx=$((($SLURM_ARRAY_TASK_ID-1)*100)) ++end_idx=$((($SLURM_ARRAY_TASK_ID)*100)) params.num_workers=2
python test.py dataset='"'$test_dataset'"' ++model_name='"'$model_name'"' ++experiment_name=$experiment_name ++model_dirpath='"'$model_dirpath'"' ++ckpt_id='"'$ckpt_id'"' ++train_dataset='"'$train_dataset'"' ++start_idx=$START  ++end_idx=$END  params.num_workers=2
 
